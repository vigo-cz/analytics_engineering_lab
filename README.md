# Analytics Engineering Lab

A comprehensive workspace for experimenting with modern data stack technologies, analytics engineering, and machine learning workflows.

## ğŸ¯ Purpose

This repository serves as a sandbox for:
- **Analytics Engineering**: dbt models, transformations, and data modeling
- **Workflow Orchestration**: Prefect-based pipeline orchestration
- **Database Technologies**: DuckDB, ClickHouse, TimescaleDB, PostgreSQL
- **Data Science & ML**: Jupyter notebooks, model training, and experiments
- **Data Quality**: Great Expectations, Soda Core, and custom validation
- **BI & Analytics**: Metabase, Lightdash dashboards
- **Data Applications**: Streamlit, Dash, Gradio apps
- **Stream Processing**: Kafka, Flink, Spark streaming

## ğŸ—ï¸ Structure

```
analytics_engineering_lab/
â”œâ”€â”€ prefect/                # Prefect flows & orchestration
â”‚   â”œâ”€â”€ flows/             # Flow definitions
â”‚   â”œâ”€â”€ deployments/       # Deployment configs
â”‚   â””â”€â”€ blocks/            # Prefect blocks
â”œâ”€â”€ dbt/                    # dbt projects (transformation layer)
â”œâ”€â”€ ingestion/              # Data extraction & loading (EL in ELT)
â”‚   â”œâ”€â”€ apis/              # API extractors
â”‚   â”œâ”€â”€ databases/         # Database extractors
â”‚   â””â”€â”€ files/             # File extractors
â”œâ”€â”€ bi/                     # BI tools & dashboards
â”‚   â”œâ”€â”€ metabase/          # Metabase configs
â”‚   â””â”€â”€ lightdash/         # Lightdash (dbt-native BI)
â”œâ”€â”€ apps/                   # Data applications
â”‚   â”œâ”€â”€ streamlit/         # Streamlit apps
â”‚   â”œâ”€â”€ dash/              # Plotly Dash apps
â”‚   â””â”€â”€ gradio/            # Gradio ML apps
â”œâ”€â”€ data-quality/           # Data quality & testing
â”‚   â”œâ”€â”€ great-expectations/ # GE suites
â”‚   â”œâ”€â”€ soda/              # Soda Core checks
â”‚   â””â”€â”€ custom/            # Custom checks
â”œâ”€â”€ databases/              # Database experiments & configs
â”œâ”€â”€ data-science/           # Ad-hoc analysis & exploratory notebooks
â”œâ”€â”€ ml/                     # Production ML systems & pipelines
â”‚   â”œâ”€â”€ models/            # Trained models
â”‚   â”œâ”€â”€ experiments/       # MLflow experiments
â”‚   â””â”€â”€ pipelines/         # ML pipelines
â”œâ”€â”€ streaming/              # Stream processing
â”œâ”€â”€ infrastructure/         # Infrastructure as Code
â”‚   â”œâ”€â”€ docker/            # Docker configs
â”‚   â””â”€â”€ terraform/         # Terraform configs
â”œâ”€â”€ scripts/                # Utility scripts
â”œâ”€â”€ tests/                  # Integration/E2E tests
â”œâ”€â”€ docs/                   # Documentation
â””â”€â”€ config/                 # Shared configurations
```

### ğŸ”¬ Data Science vs ML - What's the Difference?

**`data-science/`** - Exploratory & Ad-hoc Analysis
- ğŸ“Š Jupyter notebooks for exploratory data analysis (EDA)
- ğŸ“ˆ One-off analyses and investigations
- ğŸ“‘ Reports and findings
- ğŸ” Prototyping and experimentation
- **Temporary/exploratory** work that may not be productionized

**`ml/`** - Production ML Systems
- ğŸ¤– Production-ready trained models
- ğŸ”„ Automated ML pipelines (training, inference, retraining)
- ğŸ“Š MLflow experiment tracking and model registry
- ğŸš€ Model serving code (APIs, batch inference)
- **Long-lived, versioned, monitored** systems

**Workflow**: Start exploring in `data-science/`, then productionize valuable models in `ml/`.

## ğŸ§ª Testing Strategy

### dbt Tests (`dbt/tests/`)
- **Schema tests**: uniqueness, not_null, relationships
- **Data quality tests**: custom SQL assertions
- **Run with**: `dbt test`
- **Purpose**: Validate transformed data models

### Integration Tests (`tests/`)
- **End-to-end pipeline tests**: ingestion â†’ dbt â†’ BI
- **Prefect flow tests**: orchestration validation
- **Cross-system tests**: database connections, API availability
- **Run with**: `pytest`
- **Purpose**: Validate entire system integration

### Data Quality (`data-quality/`)
- **Great Expectations**: Comprehensive data validation
- **Soda Core**: YAML-based quality checks
- **Custom checks**: Python/SQL validation scripts
- **Purpose**: Continuous data quality monitoring

## ğŸš€ Quick Start

### Prerequisites
- Python 3.11+
- Docker & Docker Compose (optional)
- Git

### Setup
```bash
# Clone the repository
git clone <your-repo-url>
cd analytics_engineering_lab

# Set up Python virtual environment
python3.11 -m venv venv
source venv/bin/activate

# Install dependencies (per project)
cd dbt/analytics && pip install -r requirements.txt
```

## ğŸ› ï¸ Technologies

### Data Transformation
- **dbt**: Data transformation and modeling
- **SQL**: DuckDB, PostgreSQL, ClickHouse, TimescaleDB

### Orchestration
- **Prefect**: Modern workflow orchestration platform

### Data Quality
- **Great Expectations**: Data validation and profiling
- **Soda Core**: Data quality checks

### BI & Visualization
- **Metabase**: General-purpose BI tool
- **Lightdash**: dbt-native BI platform

### Data Applications
- **Streamlit**: Interactive data apps
- **Dash**: Production-grade dashboards
- **Gradio**: ML model interfaces

### Data Science & ML
- **Jupyter**: Interactive notebooks
- **scikit-learn**: Machine learning
- **pandas**: Data manipulation
- **MLflow**: ML experiment tracking

### Databases
- **DuckDB**: Embedded analytical database
- **ClickHouse**: OLAP database
- **TimescaleDB**: Time-series database
- **PostgreSQL**: Relational database

## ğŸ“š Documentation

See the [docs/](./docs/) folder for:
- Architecture diagrams
- How-to guides
- Decision records (ADRs)

Each major folder also contains its own README with specific guidance.

## ğŸ¤ Contributing

This is a personal learning lab, but feel free to fork and adapt for your own use!

## ğŸ“ License

MIT License - feel free to use and modify as needed.

